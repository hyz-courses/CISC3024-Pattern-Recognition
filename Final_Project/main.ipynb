{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea06134c-8d90-41fe-8b10-903b583fedfb",
   "metadata": {},
   "source": [
    "# **CISC3024 Pattern Recognition Final Project**\n",
    "## Group Members:\n",
    "- Huang Yanzhen, DC126732\n",
    "- Mai Jiajun, DC127853"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01653d-682d-42cd-a3e9-81c26622b8c0",
   "metadata": {},
   "source": [
    "# 0. Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42857a-bbdb-4f57-afae-b08ec4c32147",
   "metadata": {},
   "source": [
    "## 0.1 Packages & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "701b0ee5-3106-4d3f-bf0e-c511feb4a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import scipy.io as sio\n",
    "\n",
    "# Visualize Result\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score,\n",
    "                            precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score,\n",
    "                            roc_curve, auc, precision_recall_curve,\n",
    "                            average_precision_score)\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Basic\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from typing import List, Tuple, Union\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0be3ed78-d627-4715-90b8-fd957e77d4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"Using device: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe520b-0991-4f87-9782-49bf7d9f0a7d",
   "metadata": {},
   "source": [
    "## 0.2 Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffc666c2-ec1a-4b59-9e1f-997f27c72dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"./data/SVHN_mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca60f72-83da-4a78-bc7b-d2e14a0d15a7",
   "metadata": {},
   "source": [
    "# 1. Data Processing and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e8db9-06ca-4b0a-be31-e4e4a4e0531b",
   "metadata": {},
   "source": [
    "## 1.1 Download Datasets\n",
    "Define dataset class, retrieve dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410b07f-026a-42b0-bcab-c55603c25487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.1 Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4734a-61ca-481c-9758-812a25574837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_dat = sio.loadmat(os.path.join(path_dataset, \"train_32x32.mat\"))\n",
    "# _dat['X'][0][0][0]\n",
    "# np.array(_dat).shape\n",
    "dat = np.transpose(_dat['X'], (3, 0, 1, 2)).astype(np.float32)\n",
    "# dat = dat.astype(np.float32)\n",
    "dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32804a47-fea0-4bf8-ae3b-3c661a1d7bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First Image\n",
    "dat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedde86a-9db4-4e44-b851-f50a07a3e87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First Row of Image\n",
    "dat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbdf76-abd9-4f9a-80a2-a40c79404d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First Pixel of Image\n",
    "dat[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58644c-39d8-4699-8da6-a26a78e4ac96",
   "metadata": {},
   "source": [
    "### 1.2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d279dbc-5ee7-4562-89c6-5a2452d2d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    def __init__(self, mat_file, transform=None, indecies=None):\n",
    "        data = sio.loadmat(mat_file)\n",
    "        \n",
    "        self.source_file_path = mat_file\n",
    "        self.images = np.transpose(data['X'], (3, 0, 1, 2))\n",
    "        self.labels = data['y'].flatten()\n",
    "        self.labels[self.labels == 10] = 0\n",
    "        self.transform = transform\n",
    "\n",
    "        if indecies is not None:\n",
    "            self.images = self.images[indecies]\n",
    "            self.labels = self.labels[indecies]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return image, label\n",
    "\n",
    "    def get_meanstd(self):\n",
    "        images_ = self.images.astype(np.float32) / 255.0\n",
    "        mean = np.mean(images_, axis=(0,1,2))\n",
    "        std = np.std(images_, axis=(0,1,2), ddof=0)\n",
    "        \n",
    "        return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14de9667-b9b3-469e-a1b2-7b957f5cc3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.4376845359802246, 0.4437684714794159, 0.47280389070510864], std=[0.19803018867969513, 0.2010156661272049, 0.19703581929206848]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "_img = dat[0]\n",
    "_img = _transform(image=_img)['image']\n",
    "# print(_img)\n",
    "_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668236d-4b77-41b6-9a0a-23d55efaf4f8",
   "metadata": {},
   "source": [
    "## 1.2 Peak A Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f50b603-13f9-4220-9d32-57d5078599b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(dataset):\n",
    "    def unnormalize(img, mean, std):\n",
    "        \"\"\"Revert the normalization for visualization.\"\"\"\n",
    "        img = img * std + mean\n",
    "        return np.clip(img, 0, 1)\n",
    "\n",
    "    # Plotting multiple images in a grid\n",
    "    grid_rows, grid_cols = 1, 6\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(6, 6))\n",
    "    \n",
    "    peek_index = random.randint(0, dataset.__len__()-1)\n",
    "    \n",
    "    for i in range(grid_cols):\n",
    "        img_tensor, label = dataset.__getitem__(peek_index)\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n",
    "        img = unnormalize(img, norm_mean, norm_std)\n",
    "    \n",
    "        ax = axes[i]  # Get subplot axis\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Peaking data from training set of index {peak_index}.\\nImage Tnesor Size:{train_dataset.__getitem__(peak_index)[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21390835-5877-4a96-9f1a-eec0dc0a163e",
   "metadata": {},
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b2dcd-df3f-4e9a-a6e5-75e3d3a20566",
   "metadata": {},
   "source": [
    "## 2.1 Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8323ca1a-660b-4b72-999b-12413d5a52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallVGG(nn.Module):\n",
    "    def __init__(self, frame_size=32):\n",
    "        super(SmallVGG, self).__init__()\n",
    "        self.frame_size = frame_size\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 16x16\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 8x8\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # 4x4\n",
    "        )\n",
    "    \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(frame_size * 4 * 4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b889ae-e8ed-4efc-8e5e-1de2cec38251",
   "metadata": {},
   "source": [
    "## 2.2 Train and Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "519c5abf-50d8-42b2-88d9-9f33c04aba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model,\n",
    "                      train_loader,\n",
    "                      valid_loader,\n",
    "                      criterion,\n",
    "                      optimizer,\n",
    "                      num_epochs=100):\n",
    "    # Record Losses to plot\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(images)\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item() * len(images)\n",
    "\n",
    "        valid_losses.append(valid_loss / len(valid_loader))\n",
    "        print(f\"Epoch[{epoch+1}/{num_epochs}], Train Loss:{train_losses[-1]:.4f}, Validation Loss:{valid_losses[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bb3f4-ec71-48ae-a1ad-36a94e4591a6",
   "metadata": {},
   "source": [
    "## 2.3 Get Predictions\n",
    "Multiple functions are defined to evaluate data. Below is a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "def78e66-0061-4e15-9582-9886a8d88cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_path, extra_loader):\n",
    "    if not isinstance(model_path, str):\n",
    "        model_state = model_path\n",
    "    else:\n",
    "        model_state = torch.load(model_path)\n",
    "    model = SmallVGG()\n",
    "    model.load_state_dict(model_state)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    \n",
    "    pred_scores = []  # Prob. of predictions\n",
    "    true_labels = []  # Ground Truth\n",
    "    pred_labels = []  # Label of prediction, i.e., argmax(softmax(pred_scores))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(extra_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "            outputs = model(images)\n",
    "    \n",
    "            pred_scores_batch = nn.functional.softmax(outputs, dim=-1)\n",
    "    \n",
    "            pred_scores.extend(pred_scores_batch.cpu().tolist())\n",
    "            pred_labels.extend(outputs.argmax(dim=1).tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "    return pred_scores, true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce93f1-9cd6-488c-8ff5-85ae2e4b5665",
   "metadata": {},
   "source": [
    "## 2.4 Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6f80017-116d-47b6-9930-738c0bd8ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, zero_division=1, average=None, labels=range(0,10))\n",
    "    recall = recall_score(true_labels, pred_labels, zero_division=1, average=None, labels=range(0,10))\n",
    "    f1 = f1_score(true_labels, pred_labels, zero_division=0, average=None, labels=range(0,10))\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a34acf59-3f0a-43c2-8f99-57d97e429d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC AUC for each class\n",
    "def get_roc_auc(true_labels_bin, pred_labels_bin):\n",
    "    roc_auc = dict()\n",
    "    for i in range(0, 10):\n",
    "        roc_auc[i] = roc_auc_score(true_labels_bin[:,i], np.array(pred_scores)[:, i])\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3debbe-a4da-43a2-af00-91dc6925d242",
   "metadata": {},
   "source": [
    "# 3. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c0811-e928-415b-b7ca-f279d31781de",
   "metadata": {},
   "source": [
    "## 3.0 Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda24a1-6fe4-41f0-957d-883d693f9220",
   "metadata": {},
   "source": [
    "### 3.0.1 Plot Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4516265-baff-45f4-a2fa-e737737813e5",
   "metadata": {},
   "source": [
    "The experiments will be a list of the following structures:\n",
    "```python\n",
    "{\n",
    "    \"HYPER_PARAM_1\": combo[0],\n",
    "    \"HYPER_PARAM_2\": combo[1],\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"model_state_dict\": exp_model.state_dict()\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29150e7-f500-404f-909a-b1a874efc791",
   "metadata": {},
   "source": [
    "#### Epoch-Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "002c3f63-ccfc-4e70-8f11-0bc0d4731f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_el(loaded_experiments, hyper_param_names, n_rows=4, n_cols=4):\n",
    "    fig_size = (n_cols * 5, n_rows * 5)\n",
    "    n1, n2 = hyper_param_names\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=fig_size)\n",
    "    # plt.tight_layout()\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        train_losses, valid_losses = loaded_experiments[i][\"train_losses\"], loaded_experiments[i][\"test_losses\"]\n",
    "        \n",
    "        ax.plot(train_losses, label=f\"TRL, min={np.min(train_losses):.3f}\")\n",
    "        ax.plot(valid_losses, label=f\"VAL, min={np.min(valid_losses):.3f} at step={np.argmin(valid_losses)}\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(f\"{n1}={loaded_experiments[i][n1]}, {n2}={loaded_experiments[i][n2]}\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e76e8b-4a70-4518-9324-fca57ca964d0",
   "metadata": {},
   "source": [
    "#### Get Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbc2edec-3681-44a4-a918-3ddc1f399d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_results(loaded_experiments, test_hyperparam_names, extra_loader):\n",
    "    experiment_results = []\n",
    "    n1, n2 = test_hyperparam_names\n",
    "    for i, exp in enumerate(loaded_experiments):\n",
    "        pred_scores, true_labels, pred_labels = get_predictions(exp['model_state_dict'], extra_loader)\n",
    "        experiment_results.append({\n",
    "            n1: exp[n1],\n",
    "            n2: exp[n2],\n",
    "            \"true_labels\": true_labels,\n",
    "            \"pred_labels\": pred_labels,\n",
    "            \"pred_scores\": pred_scores\n",
    "        })\n",
    "\n",
    "        print(f\"First 10 true labels:\")\n",
    "        [print(num, end=\" \") for num in true_labels[:10]]\n",
    "        print(f\"...\\n\")\n",
    "\n",
    "        print(f\"First 10 pred labels:\")\n",
    "        [print(num, end=\" \") for num in pred_labels[:10]]\n",
    "        print(f\"...\\n\")\n",
    "\n",
    "        print(f\"First 5 pred_scores:\")\n",
    "        [print(num, end=\" \") for num in pred_scores[:5]]\n",
    "        print(f\"...\\n\")\n",
    "\n",
    "        # del pred_scores, true_labels, pred_lables\n",
    "        torch.cuda.empty_cache()\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539686c-69da-4fe1-bb73-e0e3426ccbee",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a25f7771-b65b-478b-aab5-d291c5d1af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(experiment_results, hyper_param_names, n_rows=4, n_cols=4):\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    hparam_1, hparam_2 = hyper_param_names\n",
    "    \n",
    "    for i, exp_rs in enumerate(experiment_results):\n",
    "        true_labels, pred_labels = exp_rs['true_labels'], exp_rs['pred_labels']\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(0,10))\n",
    "        disp.plot(ax=axes[i], cmap = plt.cm.Blues)\n",
    "        axes[i].set_title(f\"Exp {i+1}: {hparam_1}={exp_rs[hparam_1]}, {hparam_2}={exp_rs[hparam_2]}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7272cf2-b7a1-4f63-a9e1-c0411254fb4e",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6562523-28f2-4b45-bb50-dd5769b599eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr(experiment_results, hyper_param_names, n_rows=4, n_cols=4):\n",
    "    fig, axes = plt.subplots(n_rows,n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    hparam_1, hparam_2 = hyper_param_names\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for i, exp_rs in enumerate(experiment_results):\n",
    "        true_labels, pred_labels, pred_scores = exp_rs['true_labels'], exp_rs['pred_labels'], exp_rs['pred_scores']\n",
    "        true_labels_bin, pred_labels_bin = label_binarize(true_labels, classes=range(0,10)), label_binarize(pred_labels, classes=range(0,10))\n",
    "        \n",
    "        accuracy, precision, recall, f1 = get_metrics(true_labels, pred_labels)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        for j in range(0, 10):\n",
    "            # print(f\"Class {j}: Prec:{precision[j]:.2f}, Recall:{recall[j]:.2f}, F_1 Score:{f1[j]:.2f}\")\n",
    "            precision_i, recall_i, _ = precision_recall_curve(true_labels_bin[:, j], np.array(pred_scores)[:, j])\n",
    "    \n",
    "            average_precision = average_precision_score(true_labels_bin[:, j], np.array(pred_scores)[:, j])\n",
    "            axes[i].step(recall_i, precision_i, where=\"post\", label=f\"Class {j} AP={average_precision:.2f}\")\n",
    "            axes[i].set_title(f\"PR-Curve {hparam_1}={exp_rs[hparam_1]}, {hparam_2}={exp_rs[hparam_2]}\")\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel(\"Recall\")\n",
    "        axes[i].set_ylabel(\"Precision\")\n",
    "    \n",
    "    # for j in range(i+1, 16):\n",
    "    #     fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return accuracies, f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b235786-f295-485e-b7a4-bc560238a3af",
   "metadata": {},
   "source": [
    "#### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b5681fa-ef68-4045-a1b5-d9c10b7c2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rocauc(experiment_results, hyper_param_names, curve_type, n_rows=4, n_cols=4):\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    hparam_1, hparam_2 = hyper_param_names\n",
    "\n",
    "    for i, exp_rs in enumerate(experiment_results):\n",
    "        true_labels, pred_scores = exp_rs['true_labels'], exp_rs['pred_scores']\n",
    "        true_labels_bin = label_binarize(true_labels, classes=range(0, 10))\n",
    "\n",
    "        # All Classes' ROC curve & ROC Area Under Curve\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        for j in range(10):\n",
    "            fpr[j], tpr[j], _ = roc_curve(true_labels_bin[:, j], np.array(pred_scores)[:, j])\n",
    "            roc_auc[j] = auc(fpr[j], tpr[j])\n",
    "\n",
    "        # Macro-Average ROC & ROC-AUC\n",
    "        all_fpr = np.unique(np.concatenate([fpr[j] for j in range(10)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for j in range(10):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[j], tpr[j])\n",
    "        mean_tpr /= 10\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_labels_bin.ravel(), np.array(pred_scores).ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # Plot only Macro or Micro ROC curves\n",
    "        if curve_type == \"macro_micro\":\n",
    "            axes[i].plot(fpr[\"macro\"], tpr[\"macro\"], label=f\"Macro (AUC={roc_auc['macro']:.2f})\")\n",
    "            axes[i].plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"Micro (AUC={roc_auc['micro']:.2f})\")\n",
    "        elif curve_type == \"all\":\n",
    "            # Plot all ROC curves\n",
    "            for j in range(10):\n",
    "                axes[i].plot(fpr[j], tpr[j], label=f\"Class {j} (AUC={roc_auc[j]:.2f})\")\n",
    "\n",
    "        axes[i].plot([0, 1], [0, 1], \"k--\")\n",
    "        axes[i].set_xlabel(\"False Positive Rate\")\n",
    "        axes[i].set_ylabel(\"True Positive Rate\")\n",
    "        axes[i].set_title(f\"ROC Curve {i+1}, {hparam_1}={exp_rs[hparam_1]}, {hparam_2}={exp_rs[hparam_2]}\")\n",
    "        axes[i].legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c4518-c0c6-4b91-bbd0-335d0b7d4aef",
   "metadata": {},
   "source": [
    "### 3.0.1 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c885d6-2370-4569-940c-73d9b4b4ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(train_dataset, train_ratio):\n",
    "    ori_len = len(train_dataset)\n",
    "    train_size = int(train_ratio * ori_len)\n",
    "    valid_size = ori_len - train_size\n",
    "\n",
    "    # These are subsets!! Don't directly use them or you will spend 2 hours solving for it.\n",
    "    train_subset, valid_subset = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "    # Re-construct two SVHNDataset object from indecies\n",
    "    train_dataset_ = SVHNDataset(mat_file=train_dataset.source_file_path, \n",
    "                                 transform=train_dataset.transform, \n",
    "                                 indecies=train_subset.indices)\n",
    "\n",
    "    valid_dataset_ = SVHNDataset(mat_file=train_dataset.source_file_path, \n",
    "                                 transform=train_dataset.transform, \n",
    "                                 indecies=valid_subset.indices)\n",
    "    \n",
    "    return train_dataset_, valid_dataset_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d876c-8b2e-498b-bbff-c3d7d26420c3",
   "metadata": {},
   "source": [
    "## 3.1 Experiment 1: Optimizer\n",
    "In the standard process of gradient descent, each update is proportional to the negative gradient (first-order derivative) of the loss function with respect to the parameter. In this traditional process, the learning rate is fixed, and it may cause problems.\n",
    "- Oscillations. If locally, the learning rate is too high, the model will jump around the local minimum.\n",
    "- Slow convergence. If locally, the learning rate is too low, the model will spend a lot of epochs to converge to a local minimum.\n",
    "\n",
    "To solve this problem, we enable the learning rate to be adaptive by introducing the \"momentum\", a velocity-like term which accumulates past gradients in the direction of consistent descent.\n",
    "- The velocity term is the weighted sum of previous gradients.\n",
    "- ...such that the update direction does not only rely on the current gradient, but also on previous ones.\n",
    "\n",
    "The update of velocity is represnted as:\n",
    "$$\n",
    "v_t=\\beta v_{t-1} + (1-\\beta)\\cdot\\nabla J(\\theta)\n",
    "$$\n",
    "where $\\beta$ is the momentum coefficient. In our experiments, $\\beta$ will be fixed to $0.9$.\n",
    "\n",
    "The update of parameters will be:\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t-1}-\\eta\\cdot v_{t}\n",
    "$$\n",
    "In this experiment, we focus on the performance of different optimizers, each has its own optimized way to update the momentum. We will fix other variables, including transform, epoch number and learning rate, and only adjust the optimizers. There are a few optimizers to be chozen:\n",
    "\n",
    "- Adaptive Moment Estimation (Adam)\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Root Mean Square Propagation (RMSprop)\n",
    "- Adam with Weight Decay (AdamW)\n",
    "- Adaptive Gradient Algorithm (Adgrad)\n",
    "- SGD with Momentum and Nesterove Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87f11a38-81d7-4459-9156-18423cd5f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Means: [0.4376845359802246, 0.4437684714794159, 0.47280389070510864]\n",
      "Channel Stds: [0.19803018867969513, 0.2010156661272049, 0.19703581929206848]\n"
     ]
    }
   ],
   "source": [
    "# Universal Train Dataset without splitting\n",
    "exp1_universal_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"train_32x32.mat\"))\n",
    "exp1_mean, exp1_std = exp1_universal_train_dataset.get_meanstd()\n",
    "\n",
    "print(f\"Channel Means: {exp1_mean}\")\n",
    "print(f\"Channel Stds: {exp1_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81abdd-2259-4b7c-a624-2c735172ee6c",
   "metadata": {},
   "source": [
    "Define changing & non-changing hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49e946d9-fa2a-430e-b896-3abe890fc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_hyperparams = {\n",
    "    \"num_epochs\": 30,\n",
    "    \"lr\": 1e-5,\n",
    "    \"criterion\": nn.CrossEntropyLoss(),\n",
    "    \"transform\": A.Compose([\n",
    "        A.Normalize(mean=exp1_mean, std=exp1_std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}\n",
    "\n",
    "exp1_models = [SmallVGG().to(device) for _ in range(0,6)]\n",
    "\n",
    "candidate_optimizers = [\n",
    "    optim.Adam(exp1_models[0].parameters(), lr=exp1_hyperparams['lr']), \n",
    "    optim.SGD(exp1_models[1].parameters(), lr=exp1_hyperparams['lr'], momentum=0.9),\n",
    "    optim.RMSprop(exp1_models[2].parameters(), lr=exp1_hyperparams['lr']),\n",
    "    optim.AdamW(exp1_models[3].parameters(), lr=exp1_hyperparams['lr'], weight_decay=0.01),\n",
    "    optim.Adagrad(exp1_models[4].parameters(), lr=exp1_hyperparams['lr']),\n",
    "    optim.SGD(exp1_models[5].parameters(), lr=exp1_hyperparams['lr'], momentum=0.9, nesterov=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35daa0-3023-41e5-808f-67f4d9d8c119",
   "metadata": {},
   "source": [
    "Train, Validation and Test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf658a2f-ab43-4103-a7b6-c824addd62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:58605\n",
      "Validation Size:14652\n",
      "Test Size:26032\n"
     ]
    }
   ],
   "source": [
    "# Train & Test Dataset\n",
    "exp1_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"train_32x32.mat\"), transform=exp1_hyperparams['transform'])\n",
    "exp1_train_dataset, exp1_valid_dataset = split_train_valid(exp1_train_dataset, train_ratio=0.8)\n",
    "\n",
    "# Test Dataset\n",
    "exp1_test_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"test_32x32.mat\"), transform=exp1_hyperparams['transform'])\n",
    "\n",
    "print(f\"Train Size:{exp1_train_dataset.__len__()}\\nValidation Size:{exp1_valid_dataset.__len__()}\\nTest Size:{exp1_test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bb725-550b-4f46-82a0-6ee368b53216",
   "metadata": {},
   "source": [
    "Train, Validation and Test Data Loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e0dd71e-bf83-4f93-b431-5b780179ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "exp1_train_loader = DataLoader(exp1_train_dataset, batch_size=128, shuffle=True)\n",
    "exp1_valid_loader = DataLoader(exp1_valid_dataset, batch_size=128, shuffle=True)\n",
    "exp1_test_loader = DataLoader(exp1_test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4471f1-0247-4ea9-abba-2ed58aa1bc12",
   "metadata": {},
   "source": [
    "**Run Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bacc8-c132-4159-8e19-8c7d9240e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp1(optimizers, models, hyper_params, train_loader, valid_loader):\n",
    "    experiments = []\n",
    "    for i, [optimizer, exp1_model] in enumerate(zip(optimizers, models)):\n",
    "        print(f\"Experiment {i+1}. Running experiment on optimizer: {optimizer.__class__.__name__}\")\n",
    "\n",
    "        criterion = hyper_params['criterion']\n",
    "        num_epochs = hyper_params['num_epochs']\n",
    "        train_losses, test_losses = train_and_evaluate(exp1_model, train_loader, valid_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "        experiments.append({\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"others\":\"same\",\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"model_state_dict\": exp1_model.state_dict()\n",
    "        })\n",
    "\n",
    "        del exp1_model, criterion, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89cb6e-bca0-4ed1-a635-206910586c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp1 = run_exp1(candidate_optimizers, exp1_models, exp1_hyperparams, exp1_train_loader, exp1_valid_loader)\n",
    "time_str = str(time.time()).replace(\".\",\"\")\n",
    "torch.save(exp1, f\"./models/exp1_{time_str}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436291c2-a9f2-477d-9b8f-878dd777cdaa",
   "metadata": {},
   "source": [
    "**Load Experiments**\n",
    "\n",
    "Load Experiment objects and plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f31d2a-de10-46e9-927c-87b821b36303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp1_loaded = torch.load(\"./models/exp1_17302273106995156.pth\")\n",
    "exp1_results = get_experiment_results(exp1_loaded, test_hyperparam_names=[\"optimizer\", \"others\"], extra_loader=exp1_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21d10a-bbd3-4460-9420-709e6b7e6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_el(exp1_loaded, [\"optimizer\", \"others\"], n_rows=1, n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872ac39-4de4-4ffe-87a1-b3ea96b4717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(exp1_results, [\"optimizer\", \"others\"], n_rows=1, n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03e488-7869-4ed5-8b0a-78a815416286",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_accuracies, exp1_f1s = plot_pr(exp1_results, [\"optimizer\", \"others\"], n_rows=1, n_cols=6)\n",
    "print(f\"Accuracies:\")\n",
    "for acc in exp1_accuracies:\n",
    "    print(f\"{acc:.3f}\", end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"F1 Score Lists:\")\n",
    "for f1 in exp1_f1s:\n",
    "    for val in f1:\n",
    "        print(f\"{val:.3f}\", end=\", \")\n",
    "    print(f\"Avg F1={np.mean(f1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8febdb-9564-499f-8d10-e872e1fb8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocauc(exp1_results, [\"optimizer\", \"others\"], curve_type=\"all\", n_rows=1, n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a7309-d2f4-471b-a150-95e959dfb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rocauc(exp1_results, [\"optimizer\", \"others\"], curve_type=\"macro_micro\", n_rows=1, n_cols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4a571-3e60-4260-954c-402c32506315",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Experiment 2: Epoch Number and Learning Rate\n",
    "This experiment seeks to find the effect of different combinations of epoch numbers and learning rates on the training & testing performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a8c38-4099-45ec-9209-2fc0eb69da01",
   "metadata": {},
   "source": [
    "### 3.2.1 Experiment 2-1: Rough Search\n",
    "In this sub-experiment, we perform a rough search on the epochs and learning rate. We promoted four possible values for both parameters:\n",
    "$$\n",
    "\\text{candidate epochs}=\\{10, 15, 20, 25\\}\n",
    "$$\n",
    "$$\n",
    "\\text{candidate lr}=\\{1.0\\times 10^{-3},1.0\\times 10^{-4},1.0\\times 10^{-5},1.0\\times 10^{-6}\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f3633-588c-4717-aeac-aa314921500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Train Dataset without splitting\n",
    "exp2_universal_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"train_32x32.mat\"))\n",
    "exp2_mean, exp2_std = exp2_universal_train_dataset.get_meanstd()\n",
    "\n",
    "print(f\"Channel Means: {exp2_mean}\")\n",
    "print(f\"Channel Stds: {exp2_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cd371-4109-4833-a798-f0f5eb3a2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_hyperparams = {\n",
    "    \"criterion\": nn.CrossEntropyLoss(),\n",
    "    \"transform\": A.Compose([\n",
    "        A.Normalize(mean=exp2_mean, std=exp2_std),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"optimizer\":optim.Adam,\n",
    "}\n",
    "\n",
    "candidate_epochs = [10, 15, 20, 25]\n",
    "candidate_lr = [1e-3, 1e-4, 1e-5, 1e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb7d07-6923-4a55-b665-aff8eda772eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train & Validation Datasets\n",
    "exp2_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"train_32x32.mat\"), transform=exp2_hyperparams['transform'])\n",
    "exp2_train_dataset, exp2_valid_dataset = split_train_valid(exp2_train_dataset, train_ratio=0.8)\n",
    "\n",
    "# Test Dataset\n",
    "exp2_test_dataset = SVHNDataset(mat_file=os.path.join(path_dataset,\"test_32x32.mat\"), transform=exp2_hyperparams['transform'])\n",
    "\n",
    "print(f\"Train Size:{exp2_train_dataset.__len__()}\\nValidation Size:{exp2_valid_dataset.__len__()}\\nTest Size:{exp2_test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4d3c3-981e-42cb-9a42-8e93dffa2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_train_loader = DataLoader(exp2_train_dataset, batch_size=128, shuffle=True)\n",
    "exp2_valid_loader = DataLoader(exp2_valid_dataset, batch_size=128, shuffle=True)\n",
    "exp2_test_loader = DataLoader(exp2_valid_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45053908-58c5-4a68-9de8-24f70cf0743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp2(epochs, lr_list, hyper_params, train_loader, test_loader):\n",
    "    combinations = list(itertools.product(epochs, lr_list))\n",
    "    experiments = []\n",
    "    for i, combo in enumerate(combinations):\n",
    "        num_epochs, lr = combo\n",
    "\n",
    "        print(f\"Running Exp {i+1}: num_epoch={num_epochs}, lr={lr}\")\n",
    "        this_model = SmallVGG().to(device)\n",
    "        criterion = hyper_params['criterion']\n",
    "        optimizer = hyper_params['optimizer'](this_model.parameters(), lr=lr)\n",
    "        train_losses, test_losses = train_and_evaluate(this_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "        experiments.append({\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"lr\": lr,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"model_state_dict\": this_model.state_dict()\n",
    "        })\n",
    "\n",
    "        del this_model, criterion, optimizer\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b29cf-8209-4101-a53a-375c45b431cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2 = run_exp2(candidate_epochs, candidate_lr, exp2_hyperparams, exp2_train_loader, exp2_valid_loader)\n",
    "time_str = str(time.time()).replace(\".\",\"\")\n",
    "torch.save(exp2, f\"./models/exp2_1_{time_str}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8741b3e-c96d-4404-bfc9-3f28c0d68e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2_loaded = torch.load(\"./models/exp2_1_1730229238268616.pth\")\n",
    "exp2_results = get_experiment_results(exp2_loaded, test_hyperparam_names=[\"num_epochs\", \"lr\"], extra_loader=exp2_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e96e54-72e9-496a-bc89-721663ff5655",
   "metadata": {},
   "source": [
    "#### 3.2.1-1 Epoch-Loss Curve\n",
    "We found that the key to the training performance of a model is the learning rate. Epoch number only controls the progress of training.\n",
    "\n",
    "From the perspective of learning rate (each column), only the learning rate of $1.0\\times 10^{-3}$ shows a sign of convergence under each candidate epochs. With this learning rate, the model even overfitted under experiments with an epoch number over $15$. \n",
    "The best model we conclude from this rough selection is the one with the combination of $\\text{num\\_epoch}=10\\land\\text{lr}=1.0\\times10^{-3}$. The minimum validation loss is $36.648$ at step $7$, which is the lowest of all $16$ samples. However, this doesn't mean that it is optimal since it may jump over a local minimum. \n",
    "\n",
    "Moreover, as we inspect the performance on smaller learning rates, we found that they tend to converge in a way further epoch steps. Moreover, for the learning rate $1.0\\times 10^{-6}$, the learning rate is too low that the model can not even fit under nearly-finite epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f2155-6286-4412-bb4c-d581e788cadb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_el(exp2_loaded, [\"num_epochs\", \"lr\"], n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c0c0b-e478-44ae-93fc-a6be9be8228d",
   "metadata": {},
   "source": [
    "#### 3.2.1-2 Confusion Matrix\n",
    "In this rough search, the confusion matrix varies on different learning rates, and tends to be identical on different epochs.\n",
    "\n",
    "Under a same epoch number, as leraning rate gets smaller, the confusion matrix gets \"blurrer\", meaning that the prediction is less accurate from the whole perspective. The learning rates under $1.0\\times 10^{-4}$ are too low that the model can't converge in a reasonably number of epochs. For the lowest learning rate of $1.0\\times 10^{-6}$, the model is not fitted at all. It classifies every number into 1, the number with the richest amount in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8564f56-53ae-44ae-85ff-b5ef7b5bf680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cm(exp2_results, [\"num_epochs\", \"lr\"], n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0717214-23de-40cc-baa6-65ea45cd6fa7",
   "metadata": {},
   "source": [
    "#### 3.2.1-3 Precision-Recall Curve\n",
    "From a numerical perspective over the testing performance, the combination of $\\text{num\\_epoch}=10\\land\\text{lr}=1.0\\times10^{-3}$ gives the highest accuracy of $0.920$, highest average $F_1$ score of $0.916$ and the lowest $F_1$ variance per-class of $0.019$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eb74f-9f45-43c1-89a7-2ccae2515962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2_accuracies, exp2_f1s = plot_pr(exp2_results, [\"num_epochs\", \"lr\"], n_rows=4, n_cols=4)\n",
    "print(f\"Accuracies:\")\n",
    "for acc in exp2_accuracies:\n",
    "    print(f\"{acc:.3f}\", end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"F1 Score Lists:\")\n",
    "for f1 in exp2_f1s:\n",
    "    for val in f1:\n",
    "        print(f\"{val:.3f}\", end=\", \")\n",
    "    print(f\"Avg F1={np.mean(f1):.3f}, Std={np.std(f1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a20ad-2b8e-4347-b0ff-c660b1fe7ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp2_results, [\"num_epochs\", \"lr\"], curve_type=\"all\", n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3468314-9832-41e3-ac46-ef685a1c8435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp2_results, [\"num_epochs\", \"lr\"], curve_type=\"macro_micro\", n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a656d3b-37ec-4287-821a-d9595d8c4d77",
   "metadata": {},
   "source": [
    "### 3.2.2 Experiment 2-2: Detailed\n",
    "Previous sub-experiment tells that the best combination from all the listed ones is $\\text{num\\_epoch}=10 \\land \\text{lr}=1.0\\times 10^{-3}$.\n",
    "\n",
    "This is a rough solution, as it may jump over local minimums. We want to find a better learning rate around $1.0\\times 10^{-3}$, with an even more detailed distinction between candidate values, so that it may reveal a missing local minimum without using too much epochs.\n",
    "\n",
    "We conducted an excessive experiment, purposely seeking an overfitting point over the listed candidate learning rates. We do this by setting the epoch number to $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057e269-34c4-4a8f-8ebc-73912377b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_2_hyperparams = {\n",
    "    \"num_epoch\": 50,\n",
    "    \"criterion\": nn.CrossEntropyLoss(),\n",
    "    \"transform\": A.Compose([\n",
    "        A.Normalize(mean=exp2_mean, std=exp2_std),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    \"optimizer\":optim.Adam,\n",
    "}\n",
    "\n",
    "# More detailed candidate learning rates around 1e-4, that is 10e-4.\n",
    "exp2_2_candidate_lr = [17e-4, 15e-4, 15e-4, 14e-4, 13e-4, 12e-4, 8e-4, 7e-4, 6e-4, 5e-4, 4e-4, 3e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca68df-53b5-499a-af93-595838bdf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp2_2(lr_list, hyper_params, train_loader, test_loader):\n",
    "    experiments = []\n",
    "    for i, lr in enumerate(lr_list):\n",
    "\n",
    "        print(f\"Running Exp {i+1}: lr={lr}\")\n",
    "        this_model = SmallVGG().to(device)\n",
    "        num_epochs = hyper_params['num_epoch']\n",
    "        criterion = hyper_params['criterion']\n",
    "        optimizer = hyper_params['optimizer'](this_model.parameters(), lr=lr)\n",
    "        train_losses, test_losses = train_and_evaluate(this_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "        experiments.append({\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"lr\": lr,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"model_state_dict\": this_model.state_dict()\n",
    "        })\n",
    "\n",
    "        del this_model, criterion, optimizer\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087075c-0b46-4383-8ad1-bbaf0fd5fefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2_2 = run_exp2_2(exp2_2_candidate_lr, exp2_2_hyperparams, exp2_train_loader, exp2_valid_loader)\n",
    "time_str = str(time.time()).replace(\".\", \"\")\n",
    "torch.save(exp2_2, f\"./models/exp2_2_{time_str}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5240ce3-34ac-4b6d-9cea-3cd60a27c026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2_2_loaded = torch.load(\"./models/exp2_2_17302741969577262.pth\")\n",
    "exp2_2_results = get_experiment_results(exp2_2_loaded, test_hyperparam_names=[\"num_epochs\", \"lr\"], extra_loader=exp2_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220fd525-1fa6-42bf-bd1e-137cdae93dad",
   "metadata": {},
   "source": [
    "#### 3.2.2-1 Epoch-Loss Curve\n",
    "By inspecting the epoch-loss curve, we found that all the experiments are overfitted. This means that $50$ epochs are enough for conducting the detailed search. \n",
    "\n",
    "From all the detailed searches, the learning rate of $14\\times10^{-4}$, that is `1.4e-3`, yields the lowest validation loss of $36.259$ at step $5$, which is the overfitting point. We discovered a new local minimum that's been jumped over by learning rate of `1e-3`, which previously yield a validation loss of $36.648$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd12f9e-32f4-4759-bc52-81647f7d0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_el(exp2_2_loaded, [\"num_epochs\", \"lr\"], n_rows=2, n_cols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92718dc-b856-455b-b014-1515c78ffa59",
   "metadata": {},
   "source": [
    "#### 3.2.2-2 Confusion Matrix\n",
    "At a glance, from the perspective of confusion matrix, the testing performance on unknown data is roughly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671872c-e927-46bc-afe1-701cb229adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(exp2_2_results, [\"num_epochs\", \"lr\"], n_rows=2, n_cols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf815158-403f-4cdd-af52-f7824569768f",
   "metadata": {},
   "source": [
    "#### 3.2.2-3 Precision-Recall Curve\n",
    "By inspecting the evaluation metrics, we found our judgement correct. From all the over-fitted model, the model with learning rate of $7.0\\times 10^{-4}$ yields the highest accuracy of $0.909$ and the highest average per-class $F_1$ score of $0.903$. Besides, the per-class $F_1$ score is also less variant under the learning rate of $7.0\\times 10^{-4}$, with the standard deviation of $0.023$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22a908-9eaf-488c-aea2-9e69ae3683ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp2_2_accuracies, exp2_2_f1s = plot_pr(exp2_2_results, [\"num_epochs\", \"lr\"], n_rows=2, n_cols=6)\n",
    "print(f\"Accuracies:\")\n",
    "for acc in exp2_2_accuracies:\n",
    "    print(f\"{acc:.3f}\", end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"F1 Score Lists:\")\n",
    "for f1 in exp2_2_f1s:\n",
    "    for val in f1:\n",
    "        print(f\"{val:.3f}\", end=\", \")\n",
    "    print(f\"Avg F1={np.mean(f1):.3f}, Std={np.std(f1):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b2d08-e051-4ac3-a2f0-724a1cde70d2",
   "metadata": {},
   "source": [
    "#### 3.2.2-4 ROC-AUC Curve\n",
    "The ROC-AUC Curve under all the detailed candidate learning rates are roughly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918561e9-5385-4dae-a01d-2ed6e4879a02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp2_2_results, [\"num_epochs\", \"lr\"], curve_type=\"all\", n_rows=2, n_cols=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fe08f-e5e6-4529-9e06-9d72166b5a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp2_2_results, [\"num_epochs\", \"lr\"], curve_type=\"macro_micro\", n_rows=2, n_cols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08df0b-8c83-47c7-9f0b-1aa69c96f7f9",
   "metadata": {},
   "source": [
    "## 3.3 Experiment 3: Image Augmentation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699aa5a-102e-4790-ba54-ac9a19ce5cd0",
   "metadata": {},
   "source": [
    "### 3.3.1 Experiment 3-1: Rotation Angles and Crop Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5b733f7-3054-4668-9fc0-a0a906f95260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Means: [0.4376845359802246, 0.4437684714794159, 0.47280389070510864]\n",
      "Channel Stds: [0.19803018867969513, 0.2010156661272049, 0.19703581929206848]\n"
     ]
    }
   ],
   "source": [
    "exp3_universal_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset, \"train_32x32.mat\"))\n",
    "\n",
    "# The mean & std here will only be used for experiment 3-1.\n",
    "exp3_1_mean, exp3_1_std = exp3_universal_train_dataset.get_meanstd()\n",
    "\n",
    "print(f\"Channel Means: {exp3_1_mean}\\nChannel Stds: {exp3_1_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "949985f8-0e1f-4695-8fd4-3d0c74c67bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_1_hyperparams = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"lr\": 1e-3,\n",
    "    \"criterion\": nn.CrossEntropyLoss(),\n",
    "    \"optimizer\": optim.Adam,\n",
    "    \"transform\": A.Compose([\n",
    "        A.Normalize(mean=exp1_mean, std=exp1_std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Group 1\n",
    "candidate_angles = [15, 30, 45, 60]\n",
    "candidate_crops = [0.08, 0.24, 0.40, 0.60] # Left Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89edc157-54f7-409f-8835-038bc63c7a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:58605\n",
      "Validation Size:14652\n",
      "Test Size:26032\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train & Validation Datasets\n",
    "exp3_train_dataset = SVHNDataset(mat_file=os.path.join(path_dataset, \"train_32x32.mat\"))\n",
    "exp3_train_dataset, exp3_valid_dataset = split_train_valid(exp3_train_dataset, train_ratio=0.8)\n",
    "\n",
    "# Test Dataset\n",
    "exp3_test_dataset = SVHNDataset(mat_file=os.path.join(path_dataset, \"test_32x32.mat\"))\n",
    "print(f\"Train Size:{exp3_train_dataset.__len__()}\\nValidation Size:{exp3_valid_dataset.__len__()}\\nTest Size:{exp3_test_dataset.__len__()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4c67c60-cb01-4c5e-92f2-2b54793644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp3_1(angles, crops, hyper_params, train_dataset, valid_dataset):\n",
    "    combinations = list(itertools.product(angles, crops))\n",
    "    experiments = []\n",
    "    for i, combo in enumerate(combinations):\n",
    "        angle, crop = combo\n",
    "        \n",
    "        print(f\"Running Exp {i+1}: angles={angle}, crop={crop}\")\n",
    "        this_model = SmallVGG().to(device)\n",
    "        num_epochs = hyper_params['num_epochs']\n",
    "        lr = hyper_params['lr']\n",
    "        criterion = hyper_params['criterion']\n",
    "        optimizer = hyper_params['optimizer'](this_model.parameters(), lr=lr)\n",
    "\n",
    "        # Define Transform\n",
    "        this_transform = A.Compose([\n",
    "            A.RandomResizedCrop(32, 32, scale=(crop, 1.0)),\n",
    "            A.Rotate(limit=angle),\n",
    "            A.Normalize(mean=exp3_1_mean, std=exp3_1_std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "        # Generate Dataset\n",
    "        print(f\"Exp {i+1}: Generating dataset from transform\")\n",
    "        train_dataset.transform = this_transform\n",
    "        valid_dataset.transform = this_transform\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "        # Train Model\n",
    "        train_losses, test_losses = train_and_evaluate(this_model, \n",
    "                                                       train_loader, \n",
    "                                                       valid_loader, \n",
    "                                                       criterion, \n",
    "                                                       optimizer,\n",
    "                                                       num_epochs)\n",
    "\n",
    "        experiments.append({\n",
    "            \"angle\": angle,\n",
    "            \"crop\": crop,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"model_state_dict\": this_model.state_dict()\n",
    "        })\n",
    "\n",
    "        del this_model, criterion, optimizer\n",
    "        del train_loader, valid_loader\n",
    "        \n",
    "        train_dataset.transform = None\n",
    "        valid_dataset.transform = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb83a3-d9fe-4709-8ec6-7684d40f3ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp3_1 = run_exp3_1(candidate_angles, candidate_crops, exp3_1_hyperparams, exp3_train_dataset, exp3_valid_dataset)\n",
    "time_str = str(time.time()).replace(\".\",\"\")\n",
    "torch.save(exp3_1, f\"./models/exp3_1_{time_str}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6e449-b78c-49a2-a6c5-a3b7001cba1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp3_1_loaded = torch.load(\"./models/exp3_1_17302968580804682.pth\")\n",
    "exp3_1_results = get_experiment_results(exp3_1_loaded, test_hyperparam_names=[\"angle\", \"crop\"], extra_loader=exp3_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac56033-2e93-41a7-a261-4485073dd7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_el(exp3_1_loaded, [\"angle\", \"crop\"], n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2abdcf5-93ac-4c21-a2e1-e5f6ae208ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_cm(exp3_1_results, [\"angle\", \"crop\"], n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62d649-3e0f-420f-9307-400455bd5815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp3_accuracies, exp3_f1s = plot_pr(exp3_1_results, [\"angle\", \"crop\"], n_rows=4, n_cols=4)\n",
    "print(f\"Accuracies:\")\n",
    "for acc in exp3_accuracies:\n",
    "    print(f\"{acc:.3f}\", end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"F1 Score Lists:\")\n",
    "avg_f1s = []\n",
    "for f1 in exp3_f1s:\n",
    "    for val in f1:\n",
    "        print(f\"{val:.3f}\", end=\", \")\n",
    "    avg_f1s.append(np.mean(f1))\n",
    "    print(f\"Avg F1={np.mean(f1):.3f} Std F_1={np.std(f1):.3f}\")\n",
    "print(f\"Best: {np.argmax(avg_f1s)+1}-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c75f5-0d18-4c81-ad15-318cc8bbb24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp3_1_results, [\"angle\", \"crop\"], curve_type=\"all\", n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a84ef-acf1-4191-b509-374bdd7b4c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rocauc(exp3_1_results, [\"angle\", \"crop\"], curve_type=\"macro_micro\", n_rows=4, n_cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b5cd2-51f9-484b-b6e8-977326b70946",
   "metadata": {},
   "source": [
    "### 3.3.2 Experiment 3-2: Ratios & Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9399cf4-fcfb-472d-a5e7-7c247196a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_2_hyperparams = {\n",
    "    \"num_epoch\": 15,\n",
    "    \"lr\": 1e-3,\n",
    "    \"criterion\": nn.CrossEntropyLoss(),\n",
    "    \"optimizer\": optim.Adam,\n",
    "    \"crop\": 0.08,\n",
    "    \"angle\": 45\n",
    "}\n",
    "\n",
    "# Group 2\n",
    "candidate_ratios = [0.25, 0.42, 0.58, 0.75]\n",
    "candidate_channel_biases = [0, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90ee67-8fef-4347-9ffa-66df2685e355",
   "metadata": {},
   "source": [
    "Control candidates for different variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b087965-1e17-4c4e-84ea-81ea4df083ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2.1 Design Hyper Parameger\n",
    "Set the candidate hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6c5f8-d7eb-4064-b18a-37d3b32f08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_epoch_num = [20, 40, 60, 80]\n",
    "candidate_lr = [1e-3, 1e-4, 1e-5, 1e-6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5d6ed-2c64-44f8-86c7-e43e5259a433",
   "metadata": {},
   "source": [
    "From the controlled variables, generate all the possible experiment set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6434cd-9772-4779-8b7e-a2e01fcaa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = list(itertools.product(candidate_epoch_num, candidate_lr))\n",
    "for combo in combinations:\n",
    "    print(f\"[{combo[0]}, {combo[1]:.0e}]\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82627d7-ab7d-4311-8597-b18f4d405315",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2.2 Train Models\n",
    "Train the models for all the generated hyper-parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d60eb5-0b79-402f-a4c8-d7b9ba0f9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(candidate_epoch_num, candidate_lr):\n",
    "    combinations = list(itertools.product(candidate_epoch_num, candidate_lr))\n",
    "    experiments = [] # Experiment Instances\n",
    "    \n",
    "    for combo in combinations:\n",
    "        print(f\"Performing Experiment: epoch_num={combo[0]}, lr={combo[1]}\")\n",
    "        exp_model = SmallVGG().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(exp_model.parameters(), lr=combo[1])\n",
    "        num_epochs = combo[0]\n",
    "    \n",
    "        train_losses, test_losses = train_and_evaluate(exp_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "        # One experiment instance\n",
    "        experiments.append({\n",
    "            \"num_epoch\": combo[0],\n",
    "            \"lr\": combo[1],\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"model_state_dict\": exp_model.state_dict()\n",
    "        })\n",
    "        \n",
    "        # Prevent CUDA Mem Leak\n",
    "        del exp_model, criterion, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd63744-24c7-41d0-a862-7758ff9d00a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiments = run_experiment(candidate_epoch_num, candidate_lr)\n",
    "time_str = str(time.time()).replace(\".\",\"\")\n",
    "torch.save(experiments, f\"./models/experiments_{time_str}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9617ee-4d76-442a-9a00-2bfa467c3ad5",
   "metadata": {},
   "source": [
    "### 3.2.3 Load Experiments\n",
    "Load the saved experiments, and plot the epoch-loss curve to inspect training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8723786-c545-4b16-8ef4-1eb3d4b5d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_experiments = torch.load(\"./models/experiments_17296227919579012.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64538c9b-0fed-4ead-ab21-1e9fa9c2ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_el(loaded_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b80ca9-40fd-459c-8d36-149d5cd1d7df",
   "metadata": {},
   "source": [
    "### 3.2.4 Apply Model, Get Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e34f7-9770-454f-a39a-02fc44fc1729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_experiment_results(loaded_experiments, extra_loader):\n",
    "    experiment_results = []\n",
    "    for i, exp in enumerate(loaded_experiments):\n",
    "        pred_scores, true_labels_cpu, pred_labels_cpu = get_predictions(exp[\"model_state_dict\"], extra_loader)\n",
    "        print(f\"Experiment {i+1}, num_epoch={exp['num_epoch']}, lr={exp['lr']}\")\n",
    "        print(\"First 100 true labels:\")\n",
    "        [print(num, end=\" \") for num in true_labels_cpu[:100]]\n",
    "        print(\"...\\n\")\n",
    "    \n",
    "        print(\"First 100 true predictions:\")\n",
    "        [print(num, end=\" \") for num in pred_labels_cpu[:100]]\n",
    "        print(\"...\\n\")\n",
    "    \n",
    "        print(\"First 5 prediction Probabilities:\")\n",
    "        [print(num, end=\" \") for num in pred_scores[:5]]\n",
    "        print(\"...\")\n",
    "    \n",
    "        experiment_results.append({\n",
    "            \"epoch_num\": exp['num_epoch'],\n",
    "            \"lr\": exp['lr'],\n",
    "            \"true_labels\": true_labels_cpu,\n",
    "            \"pred_labels\": pred_labels_cpu,\n",
    "            \"pred_scores\": pred_scores\n",
    "        })\n",
    "    \n",
    "        del pred_scores, true_labels_cpu, pred_labels_cpu\n",
    "        torch.cuda.empty_cache()\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3e19c-0947-4a6c-b674-05a517f967c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_results = get_experiment_results(loaded_experiments, extra_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e3338-01c4-42a7-94a6-37c076b92475",
   "metadata": {},
   "source": [
    "### 3.2.5 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515c951-97ae-48a3-9155-5fae23987c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(experiment_results, ['epoch_num', 'lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313953aa-126f-486e-a2f8-ee2d91343052",
   "metadata": {},
   "source": [
    "### 3.2.6 Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf36915-a42f-486a-ac4d-562c08705ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr(experiment_results, ['epoch_num', 'lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02009137-87a0-4703-952d-3bc4fe58504a",
   "metadata": {},
   "source": [
    "### 3.2.7 ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684db32-b40e-4384-bc90-d9a98df349fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(experiment_results, ['epoch_num','lr'], \"macro_micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7d8d7-304c-4792-b986-e64875b3ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(experiment_results, ['epoch_num','lr'], \"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
